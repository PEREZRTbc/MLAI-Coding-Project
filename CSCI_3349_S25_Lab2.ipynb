{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PEREZRTbc/MLAI-Coding-Project/blob/main/CSCI_3349_S25_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgRSi9y6rroP"
      },
      "source": [
        "# Lab 2: nltk and spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxXyalbOrroV"
      },
      "source": [
        "In this lab, you will be learning a bit about how to use the Python libraries <code>nltk</code> and ``spaCy`` to perform text normalization and to explore and analyze texts.\n",
        "\n",
        "The first few parts of this notebook will help you understand how to use a Colab notebook. Note that if you ever get weird behavior in a notebook. just go up to the Runtime menu and Restart, then run each code cell up to where you started having the problem.\n",
        "\n",
        "**After** you have completed all code and questions in this notebook, you must do two things:\n",
        "\n",
        "1. Share the notebook with the me and the TAs: prudhome@bc.edu, mccullkg@bc.edu, and pawsat@bc.edu.\n",
        "2. Download this file to your computer with `File-> Download .ipynb`, move it to your repo for this lab, then add, push, and commit your version of this Colab notebook.\n",
        "\n",
        "The deadline is Monday, January 27, at 11:59pm EST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiQpOkjQrroW"
      },
      "source": [
        "## 1. Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MAa-JHcrroW"
      },
      "source": [
        "1. Make your own copy of this notebook by going to `File->Save a copy in Drive`. This will create your own copy of this notebook that you can edit and run on Colab.\n",
        "\n",
        "2. Click on the title of the notebook, up above, and change it to `YourLastName_YourFirstName_Lab2.ipynb`.\n",
        "\n",
        "3. The cell below is a code cell. Click in the cell, then type <code>print(\"Hello, World!\")</code> in that cell under the comment. Then, hit the run button just to the left of the comment that looks like a white triangle pointing to the right inside a black circle. The keyboard shortcut is <code>shift-return</code>, holding both keys down at the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U2ECI-arroW"
      },
      "outputs": [],
      "source": [
        "# enter your Hello World code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEjuGpRJrroX"
      },
      "source": [
        "Underneath your command you should now see the output <code>Hello, World!</code>.\n",
        "\n",
        "Great! Now you have run your first command in this Colab Notebook. You can always go back and edit the stuff you've written in any code cell. Just remember to re-run it if you change anything.\n",
        "\n",
        "*Note: Many notebook beginners forget that if you change the value of some variable in a block of code, that variable now has that new value everywhere -- even in earlier blocks of code. If you are having trouble, it often helps to go back and re-run the block of code where you originally set the value of that variable.*\n",
        "\n",
        "Now let's start using nltk. Type <code>import nltk</code> in the command cell below and then run the cell by clicking the run button to import the nltk library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZhbVySDrroX"
      },
      "outputs": [],
      "source": [
        "# enter your import nltk command here and hit Run\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5W9wwworroX"
      },
      "source": [
        "You might not have all the packages you need by default in nltk. Just in case, you should download the most popular ones.\n",
        "\n",
        "Note that it might take a little while to do the download. Wait until you see the little green checkmark next to the `[ ]` before you go on to the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt6AHoPArroX"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('popular')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXswg9ycrroX"
      },
      "source": [
        "Just to make sure your nltk is working, use it to calculate the minimum edit distance between two words. The function is <code>nltk.edit_distance</code> and the arguments are the two strings you want to compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp2iJcNXrroX"
      },
      "outputs": [],
      "source": [
        "# enter your call to edit distance here and hit Run\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npEetqgxrroY"
      },
      "source": [
        "We're going to be using regular expressions, so we'll import tbe `re` library, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSQM3aGNrroY"
      },
      "outputs": [],
      "source": [
        "# enter your import re command here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYkrH07ArroY"
      },
      "source": [
        "Now let's download a file. We're going to look at Great Expectations, by Charles Dickens. Click on the cell below and hit the Run button to issue the command to download the plain text version of the book from Project Gutenberg.\n",
        "\n",
        "This is one of the very cool things about Colab notebooks: you can issue (some) unix commands by preceding the command with an exclamation point. Commands involving the file system (cd, mv, etc.) are trickier in Colab. We'll learn more about how Colab interacts with the Google Drive file system in a future lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLKbaIE_rroY"
      },
      "outputs": [],
      "source": [
        "# You can put a ! before a unix command and see if it works!\n",
        "# curl is one that usually works the way you are expecting\n",
        "\n",
        "! curl -o greatexpectations.txt https://www.gutenberg.org/files/1400/1400-0.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtkVCAmgrroY"
      },
      "source": [
        "Now you have a text to work with. Again, I am punting on teaching you about the Colab vs. Google Drive file systems, but you can see that the file `greatexpectations.txt` got saved to your current directory using the unix command `ls`. You should also examine the first 100 lines and the last 100 lines of the file using `head` and `tail`. (Don't forget the exclamation point before!)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below issue unix commands preceded by ! to\n",
        "# (1) see what's in your current directory\n",
        "# (2) examine the first 100 lines of greatexpectations.txt\n",
        "# (3) examine the last 100 lines of greatexpectations.txt\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8GBqWBMLyJv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axp63bBrrroY"
      },
      "source": [
        "## 2. Loading in the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDl8e_F0rroY"
      },
      "source": [
        "You'll notice that plain text Gutenberg Project books are formatted to have 80 or fewer characters per line. This is fine for reading on an old-timey computer screen, but when we're processing text, we don't want a lot of manually inserted hard line breaks in the middle of our text. We're going to read in the text and replace line breaks with spaces. Run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWKQ9-FRrroY"
      },
      "outputs": [],
      "source": [
        "f = open(\"greatexpectations.txt\", \"r\", encoding=\"utf-8\")\n",
        "thetext = f.read().rstrip()\n",
        "thetext = re.sub(\"\\n\", \" \", thetext)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YltckFFOrroY"
      },
      "source": [
        "<code>thetext</code> is a single string containing the entire text of the book. You can see that this is true by printing out the whole thing, but that will take up lots of space. Instead just try printing a few slices like this. This prints the first 25 characters and the last 99 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-du9g2MrroY"
      },
      "outputs": [],
      "source": [
        "print(thetext[0:25])\n",
        "print(thetext[-99:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbN8HbbHrroY"
      },
      "source": [
        "Recall from when you examined the file in a text editor that there there was a bunch of text at the beginning and end of the file that was not actually a part of the text of the book. Above I showed how to use <code>re.sub</code> to remove all the line breaks. In the cell below, use <code>re.sub</code> to delete everything up to and including ``Chapter I.   `` **followed by three spaces**. Then use <code>re.sub</code> to delete everything starting from the white space that appears before ``*** END OF THE PROJECT GUTENBERG EBOOK GREAT EXPECTATIONS ***`` all the way to the end of the file.\n",
        "\n",
        "Hint: Be very careful about spaces, case, punctuation, etc. Some regular expressions you will find very useful: <code>+ ^ $ \\s .\\*</code> and <code>.\\*?</code> and the backslash.\n",
        "\n",
        "Save this updated version to a variable called `alltext`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7mTJ_SPrroZ"
      },
      "outputs": [],
      "source": [
        "# enter your code here and run it to create the alltext variable\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYMnpKGSrroZ"
      },
      "source": [
        "If you did your regular expressions right, repeating the slice printing commands above will yield the following output:\n",
        "\n",
        "<code>My father’s family name b</code><br>\n",
        "<code>the broad expanse of tranquil light they showed to me, I saw no shadow of another parting from her.</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga57HzwUrroZ"
      },
      "source": [
        "## 3. Word tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CD8hOorroZ"
      },
      "source": [
        "In Python, you can turn a \"sentence\" into a string of \"words\" by splitting on white space using the <code>split</code> function. As we've discussed in class, however, splitting on white space is not a great way to tokenize (i.e., to separate out each actual word) because you leave punctuation attached to words. This prevents you from recognizing that, for instance, \"dogs\" is the same word whether it's before a space or a comma. In addition, you won't be able to learn anything about the distribution of different punctuation marks since they will always be attached to something else.\n",
        "\n",
        "Fortunately, nltk has a word tokenizer function that, when given a string, will return a list of tokens. Here's the syntax for calling it:\n",
        "\n",
        "<code>listoftokens = nltk.word_tokenize(inputstring)</code>\n",
        "\n",
        "Call this function on <code>alltext</code> to produce a list of tokens called <code>alltokens</code>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wASD0gTfrroZ"
      },
      "outputs": [],
      "source": [
        "# call nltk.word_tokenize here and Run\n",
        "\n",
        "alltokens = nltk.word_tokenize(alltext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUdLhrC4rroZ"
      },
      "source": [
        "### Q1: How many tokens are there in this text? How many types are there in this text? What is the type:token ratio? Write three python commands in the line below that will calculate these three numbers. Then print out all three numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xE1xOblrroZ"
      },
      "outputs": [],
      "source": [
        "# line of code for token count\n",
        "\n",
        "\n",
        "\n",
        "# line of code for type count\n",
        "# (Hint: remember about sets, which you learned in CS1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# line of code for type:token ratio\n",
        "\n",
        "\n",
        "\n",
        "# line of code to print out all three\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TwTlBGwrroZ"
      },
      "source": [
        "### Q2: What text normalization might you want to do before counting the number of types and tokens? (Hint: there are some words you might be counting as separate types because of the way they are spelled even though you'd probably think of them as the same word.) How might this normalization make your type and token counts more accurate? How might it make these counts less accurate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDL44rqMrroZ"
      },
      "source": [
        "### Double click here to enter your answers to Q2\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4GUZixprroa"
      },
      "source": [
        "## 4. Frequency distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzZo8yC7rroa"
      },
      "source": [
        "Your answers to Q1 demonstrate that there must be some words that were used more than once. Suppose you want to know what are the most frequent words. You can do this using the <code>FreqDist()</code> class in nltk. Run the code below to create a frequency distribution for your list of tokens and to print out the 10 most frequent words and their counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux8UtXfQrroa"
      },
      "outputs": [],
      "source": [
        "fdist = nltk.FreqDist(alltokens)\n",
        "fdist.most_common(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRYjAHarroa"
      },
      "source": [
        "It's not too surprising that the words you see in this list are the most common words. These little words that don't add a lot of content to language but appear frequently and usually serve a specific function are called <i><b>function words</i></b> or <i><b>closed class words</i></b>. These words are important, but the don't tell us much by themselves about the story.\n",
        "\n",
        "What should we do if we want to know the most frequent words that are <i><b>content words</b></i> or <i><b>open class words</b></i> like nouns, verbs, adjectives, and adverbs -- the kinds of words that can tell us more about the story itself?\n",
        "\n",
        "We filter out the function words using a <i><b>stop list</b></i>, which is a list of words that we can skip when we're interested in the real content of a text. nltk provides a stop list that you can use and add to. Let's get it and print it out to see what's there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iomUgUAKrroa"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "print(stoplist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KC4lclsrroa"
      },
      "source": [
        "### Q3: What common and important class of tokens did you see in the most frequent token list that is missing from this list that we also might like to ignore if we are interested in content words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iVLOkr9rroa"
      },
      "source": [
        "### Double click here to answer Q3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxbou1uIrroa"
      },
      "source": [
        "Add at least three of these missing tokens to the stop list using the usual Python syntax for appending to or extended a list, and check to make sure it worked. Then make a new version of <code>alltokens</code> from which all stop words in your stoplist have been removed. Finally, create a new <code>FreqDist</code> from this stopword-free list of tokens, and print out the top 10 tokens.\n",
        "\n",
        "Keep adding stop words (or stop tokens!) to the stoplist until you start seeing mostly real content words in the top 10.\n",
        "\n",
        "**(Note: There are smart quotes in the text because it's UTF-8 not ascii. You can add these to the stoplist by just copying and pasting them into your list of things you're adding to the stoplist.)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2ulRhEbrrob"
      },
      "outputs": [],
      "source": [
        "stoplist = stopwords.words('english')\n",
        "\n",
        "# enter your code for appending at least three tokens to the stop list here\n",
        "\n",
        "\n",
        "\n",
        "# print out the stoplist to make sure your new tokens were added correctly\n",
        "\n",
        "\n",
        "\n",
        "# make a new version of alltokens called allcontenttokens that doesn't contain items from the stop list\n",
        "\n",
        "\n",
        "\n",
        "# create a new FreqDist from this new version of allcontenttokens\n",
        "\n",
        "\n",
        "\n",
        "# print out the top 10 most frequent tokens in this new FreqDist\n",
        "\n",
        "\n",
        "\n",
        "# Remember to repeat the above steps until the 10 most frequent words are content words\n",
        "# rather than function words or punctuation!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7pP3m5Mrrob"
      },
      "source": [
        "### Q4: How many tokens did you have to add to the stoplist? What do you think of nltk's stoplist?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-rzlhGDrrob"
      },
      "source": [
        "### Double click here to answer Q4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p9bgugArrob"
      },
      "source": [
        "## 5. N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKWKWkHerrob"
      },
      "source": [
        "In class, we learned about language modeling with n-grams. An n-gram is sequence of *n* words. When $n=1$, you have unigrams. When $n=2$, you have bigrams. When $n=3$, you have trigrams, and so on. You can learn a lot about a language just by looking at the frequencies of n-grams.\n",
        "\n",
        "You can count n-grams easily yourself, but the nltk library makes counting n-grams really easy with <code>nltk.util.ngrams</code>.\n",
        "\n",
        "The `nltk.util.ngrams()` function creates a `generator` object containing all of bigrams and trigrams. That might be helpful if we were dealing with a lot of data, but it's really not necessary here. I am converting the generator objects into lists so we can look at them and do stuff with them. You can just run the code cell below.\n",
        "\n",
        "**Note: Below I am using ``alltokens`` here and not ``allcontenttokens``. Why? Because language models are used when we are interested in word sequences and how words fit together with each other. We don't want to throw out the function words since they tell us about the structure of the language.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUdHRqqlrrob"
      },
      "outputs": [],
      "source": [
        "mybigrams = nltk.ngrams(alltokens,2)\n",
        "mytrigrams = nltk.ngrams(alltokens,3)\n",
        "\n",
        "print(type(mybigrams))\n",
        "print(type(mytrigrams))\n",
        "\n",
        "bigramlist = list(mybigrams)\n",
        "trigramlist = list(mytrigrams)\n",
        "\n",
        "print(bigramlist[:10])\n",
        "print(trigramlist[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyqh2i1Trrob"
      },
      "source": [
        "Make a `FreqDist` for the bigrams above passing in `bigramlist`. Then do the same for trigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYUZFNsurrob"
      },
      "outputs": [],
      "source": [
        "# Enter your code for creating FreqDist for bigramlist and trigramlist.\n",
        "\n",
        "\n",
        "\n",
        "# Print out the 10 most common bigrams and trigrams.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW7eFZlArrob"
      },
      "source": [
        "When working with bigrams in nltk, you can also build a <i>conditional</i> frequency distribution, which, for a given word, keeps track of the frequencies of any following words. Let's look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUAu6mtkrrob"
      },
      "outputs": [],
      "source": [
        "bicfreq = nltk.ConditionalFreqDist(nltk.ngrams(alltokens,2))\n",
        "\n",
        "print(bicfreq[\"Mr.\"].most_common(10))  # prints out common words after Mr.\n",
        "\n",
        "print(bicfreq[\"who\"].most_common(10))  # prints out common words after who\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghg9dTATrrob"
      },
      "source": [
        "## 6. Stemming and lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWkDyTI6rrob"
      },
      "source": [
        "There's a common normalization task we haven't performed yet: stemming or lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtEmW0S_rroc"
      },
      "source": [
        "### Q5: Looking at the top 50 or 100 most frequent unigrams, how can you tell the tokens are not stemmed or lemmatized?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zQZvtFWrroc"
      },
      "source": [
        "### Double click here to answer Q5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-we3U76rroc"
      },
      "source": [
        "The command cell below shows how to use nltk's only true lemmatizer, the WordNet Lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE6ILI9Wrroc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# an example\n",
        "print(lemmatizer.lemmatize(\"dogs\"))\n",
        "print(lemmatizer.lemmatize(\"speaks\", \"v\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cWv7OVWrroc"
      },
      "source": [
        "Use this lemmatizer to lemmatize every token in the <code>allcontenttokens</code> list you created above. Then make a new frequency distribution and examine the 50 or 100 most frequent words.\n",
        "\n",
        "**Note: I want you to use ``allcontenttokens`` here. Why? Because we are thinking about words by themselves here rather than word sequences so we can disregard function words. In addition, you can lemmatize only verbs, nouns, adjectives, and adverbs (in English, at least).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4AK8EySrroc"
      },
      "outputs": [],
      "source": [
        "# Create a new list of tokens, all_lemmas by lemmatizing allcontenttokens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build a new FreqDist on all_lemmas and print out the 50\n",
        "# most frequent lemmatized tokens.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpFaq1y3rroc"
      },
      "source": [
        "It probably doesn't look much better. This is because the WordNet lemmatizer in nltk assumes by default that every word is a noun. Unless you tell the lemmatizer that something is a verb, it won't try to look it up as a verb. This is why \"said\" doesn't get lemmatized, and also why \"was\" gets lemmatized to \"wa\". In a future lab or problem set, we'll be exploring automatic part of speech tagging, which allows us to label every word as a noun, verb, adjective, preposition, etc. We'll also see shortly that spaCy does a much better job with this.\n",
        "\n",
        "Note that there are several different stemmers implemented in the nltk.stem package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FoAAoYIrroc"
      },
      "source": [
        "## 7. Sentence tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iik4fnlKrroc"
      },
      "source": [
        "For the second part of this lab, you'll need to take this text and save it out to a file with one tokenized sentence per line. Let's start by going back to the string holding our original text, <code>alltext</code>. We can turn this into a list of strings, each of which is a sentence, using the <code>sent_tokenize()</code> function, which takes a string as an argument and returns a list of sentences.\n",
        "\n",
        "Below, take <code>alltext</code>, break it up into sentences with <code>sent_tokenize()</code>. Then loop through the sentences in that list, and use <code>word_tokenize</code> to tokenize each sentence. Print each tokenized sentence out to a file so that you have one sentence per line.\n",
        "\n",
        "**Note: Do not just call <code>print()</code>! This will print out an ugly list of lists. Cycle through the lists to print out strings.**\n",
        "\n",
        "For example, this text:\n",
        "\n",
        "<code>Open the pod bay doors, Hal! I'm sorry, Dave. I can't do that.</code>\n",
        "\n",
        "would get printed to a file as this:\n",
        "\n",
        "<code>Open the pod bay doors , Hal !</code><br>\n",
        "<code>I'm sorry , Dave .</code><br>\n",
        "<code>I can't do that .</code><br>\n",
        "\n",
        "Please observe that there are *no quotes, no square brackets, and no commas, as you would get if you just called ``print()`` on a Python list!* TAs will be instructed to give you a 0 for this section if you print out a raw list.\n",
        "\n",
        "Name the file you print out to <code>great.txt</code>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4AW88KRErroc"
      },
      "outputs": [],
      "source": [
        "# use sent_tokenize() to break alltext into a list of sentences, allsent\n",
        "\n",
        "\n",
        "\n",
        "# Open a file to write to called great.txt.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loop through the sentences\n",
        "\n",
        "\n",
        "    # call word_tokenize() on each sentence\n",
        "\n",
        "\n",
        "    # First write out <s> to the file to indicate the beginning of a sentence, then a space.\n",
        "\n",
        "\n",
        "    # Then write out to the file each token one-by-one, each followed by a space.\n",
        "\n",
        "\n",
        "    # Then write out </s> to indicate the end of the sentence.\n",
        "\n",
        "\n",
        "\n",
        "# Close the file great.txt if needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c802862hrroc"
      },
      "source": [
        "Look at the first 10 and last 10 lines of your new `great.txt` file using unix commands in the cell below.\n",
        "\n",
        "The second line of your file should look like this:\n",
        "\n",
        "```\n",
        "<s> So , I called myself Pip , and came to be called Pip . </s>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use unix commands to look at the first 10 and last 10 lines.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2J_nRrTj-GHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj-kuZ8-rroc"
      },
      "source": [
        "## 8. Using spaCy to do a lot of this work for you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jru_cIV9rroc"
      },
      "source": [
        "It is crucial that you understand how to do each of these steps yourself since which steps you do depends very much on what task you are working on. Things like capitalization, punctuation, function words, and sentence boundaries might be important for what you want to do, or they might not matter at all.\n",
        "\n",
        "However, there is a different python library, spaCy, that will do a lot of this for you (and more!) automagically (and more slowly). Experiment with the code below to see the different things spaCy can do. To explore more, you can consult [the official spaCy documentation](https://spacy.io/api) or helpful websites like [this](https://realpython.com/natural-language-processing-spacy-python/).\n",
        "\n",
        "**You might get a warning or other feedback saying you need to restart your runtime. I was able to get everything to work without restarting, but if you can't run the subsequent code, just restart and rerun (`Runtime -> Restart Session and Run All`).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XUnj6y_rroc"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# This line below will download the English models.\n",
        "# They appear to already be available on Colab, but if you run into\n",
        "# problems you can uncomment this line.\n",
        "# You may also need to download models when you get to the part\n",
        "# below where you look at another language. You can use the code\n",
        "# below as a model.\n",
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "# This line loads a big model/pipeline that works specifically for English.\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Remember: spaCy is fancy, so it can be slow. Let's look at just the\n",
        "# first 10000 characters of Great Expectations.\n",
        "\n",
        "doc = nlp(alltext[0:10000])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFeW_VgJrrod"
      },
      "source": [
        "The pipeline you loaded in line 9 in the above code block, which I called ``nlp``, takes as input a text. It then returns a data structure that contains a very detailed processing and analysis of that text, including sentence boundary detection, tokenization, lemmatizing, part of speech  tagging, and all kinds of other helpful things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5db0Oww4rrod"
      },
      "outputs": [],
      "source": [
        "# Here's how to get access to sentences.\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn6R1FRxrrod"
      },
      "outputs": [],
      "source": [
        "# Here's how to get tokens, along with information\n",
        "# about each token such as its lemma and part of speech.\n",
        "# I am just printing out the first 10!\n",
        "for token in doc[:10]:\n",
        "    print(token, token.lemma_, token.pos_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kBDMPMgPrrod"
      },
      "outputs": [],
      "source": [
        "# spaCy has stoplists, too, and they are much more\n",
        "# complete and expansive (perhaps too expansive)\n",
        "# than the nltk list\n",
        "\n",
        "english_stops = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print(english_stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B07fH0z6rrod"
      },
      "source": [
        "To get some practice using spaCy, I'd like you to try it out some of the above commands but with a new text in a language that is *not* English!\n",
        "\n",
        "**Step 1**: There are many trained pipelines for different languages here: https://spacy.io/models. Go to that address, and pick a language. In the code box below where you pick the language, you'll see a line that shows you how to load a pipeline for the language you have selected (e.g., ``nlp = spacy.load(\"es_core_news_sm\")`` for Spanish).\n",
        "\n",
        "**Step 2**: Go on the web and find a chunk of text for the language you chose. You can pick text from Gutenberg or from Google news for that langauge or from any website where you can get a good continuous chunk of 100-200 words.\n",
        "\n",
        "**Step 3**: Process that chunk of text with the language pipeline you chose in Step 1. (You can just copy and paste the chunk into your code block.)\n",
        "\n",
        "**Step 4**: Print out the following in a pleasing way (i.e., no raw lists or other data structures):\n",
        "* The number of tokens in the text.\n",
        "* The number of sentences in the text.\n",
        "* All the verbs in the text, with each verb being printed only once.\n",
        "* All the stopwords in the text, with each stopword being printed only once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atM4-LqFrrod"
      },
      "outputs": [],
      "source": [
        "# Write your code for Part 8 here. Do not forget to include comments!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mllqXpqrrod"
      },
      "source": [
        "## 9. Verifying and submitting your work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxc7aH7Trrod"
      },
      "source": [
        "1. Make sure you've answered every <b>Q</b> question.\n",
        "\n",
        "2. Make sure you've written code wherever required.\n",
        "\n",
        "3. Go up to the `Runtime` menu and select `Restart Session and Run All`. This will run all of the code you've written. Make sure there are no errors.\n",
        "\n",
        "4. Download this Colab notebook to your computer by going to `File -> Download -> Download .ipynb`.\n",
        "\n",
        "5. Move your downloaded notebook to your lab 2 repo.\n",
        "\n",
        "6. Add, commit, and push.\n",
        "\n",
        "7. Share the notebook with the me and the TAs: prudhome@bc.edu, mccullkg@bc.edu, and pawsat@bc.edu.\n",
        "\n",
        "This lab is due January 27 at 11:59pm EST."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}